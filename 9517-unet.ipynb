{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7580641,"sourceType":"datasetVersion","datasetId":3953883}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision\n!pip install segmentation-models-pytorch\n!pip install pycocotools","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T01:55:05.419012Z","iopub.execute_input":"2024-10-31T01:55:05.420251Z","iopub.status.idle":"2024-10-31T01:55:47.457586Z","shell.execute_reply.started":"2024-10-31T01:55:05.420194Z","shell.execute_reply":"2024-10-31T01:55:47.456626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Path to annotations JSON file\nannotations_file = '/kaggle/input/seaturtleid2022/turtles-data/data/annotations.json'\n\n# Load the JSON data\nwith open(annotations_file, 'r') as f:\n    annotations = json.load(f)\n\n# Step 1: Check the top-level keys\nprint(\"Top-level keys:\", annotations.keys())\n\n# Step 2: Inspect the first few entries in the 'annotations' section\nprint(\"Number of annotations:\", len(annotations['annotations']))\nprint(\"First annotation entry:\", json.dumps(annotations['annotations'][0], indent=4))\n\n# Inspect the first few entries in the 'images' section\nprint(\"First images entry:\", json.dumps(annotations['images'][0], indent=4))","metadata":{"execution":{"iopub.status.busy":"2024-10-31T01:55:47.460043Z","iopub.execute_input":"2024-10-31T01:55:47.460735Z","iopub.status.idle":"2024-10-31T01:55:54.622962Z","shell.execute_reply.started":"2024-10-31T01:55:47.460688Z","shell.execute_reply":"2024-10-31T01:55:54.622007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\n# Access the list of annotation entries\nannotation_entries = annotations['annotations']  # Now this should be a list of dictionaries\n\n# Extract the category_id for each annotation\ncategory_ids = [annotation['category_id'] for annotation in annotation_entries]\n\n# Count the occurrences of each category_id\ncategory_counts = Counter(category_ids)\n\n# Display the result\nprint(\"Category ID Counts:\")\nfor category_id, count in category_counts.items():\n    print(f\"Category ID {category_id}: {count} annotations\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T01:55:54.624242Z","iopub.execute_input":"2024-10-31T01:55:54.624691Z","iopub.status.idle":"2024-10-31T01:55:54.639721Z","shell.execute_reply.started":"2024-10-31T01:55:54.624644Z","shell.execute_reply":"2024-10-31T01:55:54.638805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming annotations is the main dictionary containing all dataset information\ncategory_mapping = {category['id']: category['name'] for category in annotations['categories']}\n\n# Display the category mapping\nprint(\"Category ID Mapping:\")\nfor category_id, name in category_mapping.items():\n    print(f\"Category ID {category_id}: {name}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T01:55:54.642344Z","iopub.execute_input":"2024-10-31T01:55:54.642729Z","iopub.status.idle":"2024-10-31T01:55:54.652683Z","shell.execute_reply.started":"2024-10-31T01:55:54.642684Z","shell.execute_reply":"2024-10-31T01:55:54.651810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Base directory where the images are stored\nimage_base_dir = '/kaggle/input/seaturtleid2022/turtles-data/data/images/'\n\n# Load all image paths (recursive)\nimage_paths = []\nfor root, dirs, files in os.walk(image_base_dir):\n    for file in files:\n        if file.endswith('.jpg') or file.endswith('.png') or file.endswith('.JPG') or file.endswith('.jpeg'):  # Adjust file formats as needed\n            image_paths.append(os.path.join(root, file))\n\nprint(f\"Found {len(image_paths)} images.\")\nprint(image_paths[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-31T01:55:54.653754Z","iopub.execute_input":"2024-10-31T01:55:54.654031Z","iopub.status.idle":"2024-10-31T01:55:57.264307Z","shell.execute_reply.started":"2024-10-31T01:55:54.654001Z","shell.execute_reply":"2024-10-31T01:55:57.263415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a mapping from image_id to file_name from the 'images' section\nimage_id_to_filename = {}\nfor image_info in annotations['images']:\n    image_id = image_info['id']           # Image ID in 'images' section\n    file_name = image_info['file_name']   # Corresponding file name\n    image_id_to_filename[image_id] = file_name\n\n# Create a mapping from file names to lists of annotations using image_id\nannotations_dict = {}\nfor annotation in annotations['annotations']:\n    image_id = annotation['image_id']  # Get image_id from the annotation\n    image_file_name = image_id_to_filename[image_id]  # Look up file name using image_id\n\n    # Initialize a list if the image has no annotations yet\n    if image_file_name not in annotations_dict:\n        annotations_dict[image_file_name] = []\n\n    # Append each annotation to the corresponding imageâ€™s list of annotations\n    annotations_dict[image_file_name].append(annotation)\n\n# Print some sample data\nprint(f\"Number of unique images with annotations: {len(annotations_dict)}\")\nprint(f\"Example annotations for {list(annotations_dict.keys())[0]}:\")\nfor ann in annotations_dict[list(annotations_dict.keys())[0]]:\n    print(ann)\n\n# Find images that have no annotations\nimages_without_annotations = [file_name for file_name in image_id_to_filename.values() if file_name not in annotations_dict]\n\n# Print the filenames of images without annotations\nprint(f\"\\nNumber of images without annotations: {len(images_without_annotations)}\")\nprint(\"Filenames of images without annotations:\")\nfor file_name in images_without_annotations:\n    print(file_name)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T01:55:57.265743Z","iopub.execute_input":"2024-10-31T01:55:57.266121Z","iopub.status.idle":"2024-10-31T01:55:57.323020Z","shell.execute_reply.started":"2024-10-31T01:55:57.266077Z","shell.execute_reply":"2024-10-31T01:55:57.322145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Display each of the images without annotations.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nfrom pycocotools.coco import COCO\nimport os\n\n# Define file paths\nannotations_file = '/kaggle/input/seaturtleid2022/turtles-data/data/annotations.json'\nimage_base_dir = '/kaggle/input/seaturtleid2022/turtles-data/data/'\n\n# Initialize COCO API\ncoco = COCO(annotations_file)\n\n# List of filenames with no annotations\nimage_filenames_without_annotations = [\n    'images/t221/wXxaQqJIQN.JPG',\n    'images/t522/xulOLaCSKj.JPG',\n    'images/t500/YFCPGMJkJg.jpeg'\n]\n\n# Loop through each image with no annotations\nfor filename in image_filenames_without_annotations:\n    # Find the image ID based on the filename\n    image_id = None\n    for img in coco.imgs.values():\n        if img['file_name'] == filename:\n            image_id = img['id']\n            break\n\n    if image_id is None:\n        print(f\"Image {filename} not found in COCO dataset.\")\n        continue\n\n    # Load and display the original image\n    image_path = os.path.join(image_base_dir, filename)\n    image = np.array(Image.open(image_path))\n\n    # Retrieve all annotation IDs for the given image (should be empty)\n    anns_ids = coco.getAnnIds(imgIds=image_id, iscrowd=None)\n    anns = coco.loadAnns(anns_ids)\n    \n    print(f'Number of annotations for image {filename} = {len(anns)}')\n\n    # Determine number of subplots\n    if len(anns) > 0:\n        num_plots = len(anns) + 1\n    else:\n        num_plots = 1\n\n    fig, axs = plt.subplots(1, num_plots, figsize=(15, 5))\n\n    # Ensure axs is always an array for consistency\n    if num_plots == 1:\n        axs = [axs]  # Make axs a list if it's a single plot\n\n    axs[0].imshow(image)\n    axs[0].set_title(\"Original Image\")\n    axs[0].axis(\"off\")\n\n    # If there are no annotations, show a blank mask with a message\n    if len(anns) == 0:\n        blank_mask = np.zeros_like(image)  # Create a blank mask with the same shape as the image\n        axs[0].imshow(blank_mask, alpha=0.5)  # Overlay a blank mask for clarity\n        axs[0].set_title(\"No Annotations\")\n    else:\n        # Loop over each annotation and visualize it separately\n        for i, ann in enumerate(anns):\n            # Generate the binary mask for the current annotation\n            mask = coco.annToMask(ann)\n\n            # Create a copy of the original image and apply the mask\n            masked_image = image.copy()\n            masked_image[mask == 0] = 0  # Set all non-annotation areas to black\n\n            # Display each annotated region with the corresponding category name\n            axs[i + 1].imshow(masked_image)\n            category_name = coco.loadCats(ann['category_id'])[0]['name']\n            axs[i + 1].set_title(f\"Category: {category_name}\")\n            axs[i + 1].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T01:55:57.324237Z","iopub.execute_input":"2024-10-31T01:55:57.324542Z","iopub.status.idle":"2024-10-31T01:56:06.856375Z","shell.execute_reply.started":"2024-10-31T01:55:57.324510Z","shell.execute_reply":"2024-10-31T01:56:06.855433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"View an image with all of its annotations (note that turtle annotation covers entire turtle, but we will need to change the corresponding mask to just cover the carapace (body)).","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nfrom pycocotools.coco import COCO\n\n# Define file paths\nannotations_file = '/kaggle/input/seaturtleid2022/turtles-data/data/annotations.json'\nimage_base_dir = '/kaggle/input/seaturtleid2022/turtles-data/data/'\n\n# Initialize COCO API\ncoco = COCO(annotations_file)\n\n# Set the image ID you want to visualize\nimage_id = 7  # Change this to the ID of the image you want to visualize\nimg = coco.loadImgs(image_id)[0]\n\n# Load and display the original image\nimage_path = os.path.join(image_base_dir, img['file_name'])\nimage = np.array(Image.open(image_path))\n\n# Retrieve all annotation IDs for the given image\ncat_ids = coco.getCatIds()\nanns_ids = coco.getAnnIds(imgIds=img['id'], catIds=cat_ids, iscrowd=None)\nanns = coco.loadAnns(anns_ids)\n\n# Plot the original image\nfig, axs = plt.subplots(1, len(anns) + 1, figsize=(15, 5))\naxs[0].imshow(image)\naxs[0].set_title(\"Original Image\")\naxs[0].axis(\"off\")\n\n# Loop over each annotation and visualize it separately\nfor i, ann in enumerate(anns):\n    # Generate the binary mask for the current annotation\n    mask = coco.annToMask(ann)\n    \n    # Create a copy of the original image and apply the mask\n    masked_image = image.copy()\n    masked_image[mask == 0] = 0  # Set all non-annotation areas to black\n\n    # Display each annotated region with the corresponding category name\n    axs[i + 1].imshow(masked_image)\n    category_name = coco.loadCats(ann['category_id'])[0]['name']\n    axs[i + 1].set_title(f\"Category: {category_name}\")\n    axs[i + 1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T01:56:06.857646Z","iopub.execute_input":"2024-10-31T01:56:06.857969Z","iopub.status.idle":"2024-10-31T01:56:16.678808Z","shell.execute_reply.started":"2024-10-31T01:56:06.857935Z","shell.execute_reply":"2024-10-31T01:56:16.677749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load and split the data using metadata csv","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Load the metadata_splits.csv file\nmetadata_file = '/kaggle/input/seaturtleid2022/turtles-data/data/metadata_splits.csv'\nmetadata_df = pd.read_csv(metadata_file)\n\n# Step 2: Pick split column\nsplit_column = 'split_open'\n\n# Step 3: Create the train, valid, and test splits based on the selected split column\ntrain_df = metadata_df[metadata_df[split_column] == 'train']\nvalid_df = metadata_df[metadata_df[split_column] == 'valid']\ntest_df = metadata_df[metadata_df[split_column] == 'test']\n\n# Filter each split to include only images that have annotations\ntrain_image_paths = [img for img in train_df['file_name'] if img in annotations_dict]\nvalid_image_paths = [img for img in valid_df['file_name'] if img in annotations_dict]\ntest_image_paths = [img for img in test_df['file_name'] if img in annotations_dict]\n\n# Create filtered annotation dictionaries for each split, keeping the list of annotations\ntrain_annotations = {k: annotations_dict[k] for k in train_image_paths if k in annotations_dict}\nvalid_annotations = {k: annotations_dict[k] for k in valid_image_paths if k in annotations_dict}\ntest_annotations = {k: annotations_dict[k] for k in test_image_paths if k in annotations_dict}\n\n# Count the total number of annotations in each split\ntrain_annotation_count = sum(len(ann_list) for ann_list in train_annotations.values())\nvalid_annotation_count = sum(len(ann_list) for ann_list in valid_annotations.values())\ntest_annotation_count = sum(len(ann_list) for ann_list in test_annotations.values())\n\n# Print the split summary\nprint(f\"Training set contains {len(train_annotations)} images and {train_annotation_count} annotations.\")\nprint(f\"Validation set contains {len(valid_annotations)} images and {valid_annotation_count} annotations.\")\nprint(f\"Test set contains {len(test_annotations)} images and {test_annotation_count} annotations.\")\n\nprint(f\"Total images in metadata: {len(metadata_df)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T01:56:16.680555Z","iopub.execute_input":"2024-10-31T01:56:16.680992Z","iopub.status.idle":"2024-10-31T01:56:17.699853Z","shell.execute_reply.started":"2024-10-31T01:56:16.680947Z","shell.execute_reply":"2024-10-31T01:56:17.698950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create custom dataset class","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nimport pycocotools.mask as maskUtils\nimport numpy as np\nimport os\n\nclass TurtleSegmentationDataset(Dataset):\n    def __init__(self, image_paths, annotations_dict, transform=None, image_size=(256, 256), base_dir='/kaggle/input/seaturtleid2022/turtles-data/data/'):\n        self.transform = transform\n        self.image_size = image_size\n        self.base_dir = base_dir\n        self.image_paths = []\n        self.annotations = []\n\n        # Collect paths and annotations\n        for img_path in image_paths:\n            if img_path in annotations_dict and os.path.isfile(os.path.join(self.base_dir, img_path)):\n                self.image_paths.append(img_path)\n                self.annotations.append(annotations_dict[img_path])\n            else:\n                print(f\"Annotation not found for image: {img_path}\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        full_path = os.path.join(self.base_dir, img_path)\n        image = Image.open(full_path).convert(\"RGB\").resize(self.image_size)\n\n        # Initialize masks for each category\n        mask_head = np.zeros((self.image_size[0], self.image_size[1]), dtype=np.uint8)\n        mask_flippers = np.zeros((self.image_size[0], self.image_size[1]), dtype=np.uint8)\n        mask_turtle = np.zeros((self.image_size[0], self.image_size[1]), dtype=np.uint8)\n\n        # Apply each annotation's mask to the correct category-specific mask\n        for annotation in self.annotations[idx]:\n            category_id = annotation['category_id']\n            segmentation = annotation['segmentation']\n\n            # Check if segmentation is in uncompressed RLE format\n            if isinstance(segmentation, dict) and 'counts' in segmentation and isinstance(segmentation['counts'], list):\n                # Convert uncompressed RLE to compressed RLE\n                rle = maskUtils.frPyObjects([segmentation], *segmentation['size'])[0]\n            else:\n                rle = segmentation  # Already in compressed format\n\n            # Decode the segmentation mask (RLE encoded)\n            rle_mask = maskUtils.decode(rle)\n            rle_mask_resized = np.array(Image.fromarray(rle_mask).resize(self.image_size, Image.NEAREST))\n\n            # Assign each category to its respective mask\n            if category_id == 1:  # Turtle body\n                mask_turtle[rle_mask_resized == 1] = 1\n            elif category_id == 2:  # Flippers\n                mask_flippers[rle_mask_resized == 1] = 2\n            elif category_id == 3:  # Head\n                mask_head[rle_mask_resized == 1] = 3\n\n        # Combine masks: head and flippers take precedence over the turtle body\n        mask = np.maximum(mask_turtle, mask_flippers)  # Ensure flippers overlay body\n        mask = np.maximum(mask, mask_head)  # Ensure head overlays both body and flippers\n\n        # Convert image and mask to tensors\n        image = transforms.ToTensor()(image)\n        mask = torch.from_numpy(mask).long()  # Use long for CrossEntropyLoss\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, mask","metadata":{"execution":{"iopub.status.busy":"2024-10-31T01:56:17.702746Z","iopub.execute_input":"2024-10-31T01:56:17.703046Z","iopub.status.idle":"2024-10-31T01:56:22.551163Z","shell.execute_reply.started":"2024-10-31T01:56:17.703013Z","shell.execute_reply":"2024-10-31T01:56:22.550342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define UNet model","metadata":{}},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\nimport torch.optim as optim\nimport torch.nn as nn\n\n# Define the U-Net model with a pre-trained ResNet34 backbone\nmodel = smp.Unet(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",     # Use ImageNet pre-trained weights\n    in_channels=3,                  # Input channels (RGB)\n    classes=4,                      # Output classes (background, head, body, flippers)\n    activation=None           # No softmax applied here; handled by CrossEntropyLoss\n)\n\n\n# Define the loss function and the optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n\n# Print model summary (optional)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T01:56:22.552516Z","iopub.execute_input":"2024-10-31T01:56:22.553069Z","iopub.status.idle":"2024-10-31T01:56:27.410084Z","shell.execute_reply.started":"2024-10-31T01:56:22.553029Z","shell.execute_reply":"2024-10-31T01:56:27.409040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets observe an image-mask pair to make sure that the body has been successfully separated from the head and flippers.","metadata":{}},{"cell_type":"code","source":"# Create train dataset and take first element out of it\ntrain_dataset = TurtleSegmentationDataset(train_image_paths, train_annotations, image_size=(256, 256))\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom PIL import Image\n\n# Choose an index to visualize\nindex = 0  # Change this to visualize a different image-mask pair\n\n# Load the image and mask from the dataset\nimage, mask = train_dataset[index]  # Assuming train_dataset is already initialized as TurtleSegmentationDataset\n\n# Convert the image tensor back to a numpy array for visualization\nimage_np = image.permute(1, 2, 0).cpu().numpy()  # Convert CxHxW to HxWxC for plotting\nmask_np = mask.cpu().numpy()  # Convert mask tensor to numpy\n\n# Plotting\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\naxs[0].imshow(image_np)\naxs[0].set_title(\"Original Image\")\naxs[0].axis(\"off\")\n\n# Visualize the mask by using a color map (cmap) for clarity\naxs[1].imshow(mask_np, cmap=\"tab10\", vmin=0, vmax=3)  # Use discrete colors for classes\naxs[1].set_title(\"Segmentation Mask (0: Background, 1: Turtle, 2: Flipper, 3: Head)\")\naxs[1].axis(\"off\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T01:56:27.411500Z","iopub.execute_input":"2024-10-31T01:56:27.411901Z","iopub.status.idle":"2024-10-31T01:56:30.884674Z","shell.execute_reply.started":"2024-10-31T01:56:27.411857Z","shell.execute_reply":"2024-10-31T01:56:30.883739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets implement the training loop","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch\nimport time\n\n# Create datasets\ntrain_dataset = TurtleSegmentationDataset(train_image_paths, train_annotations, image_size=(256, 256))\nvalid_dataset = TurtleSegmentationDataset(valid_image_paths, valid_annotations, image_size=(256, 256))\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers = 2)\nvalid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False, num_workers = 2)\n\n# Move model to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Training loop\nnum_epochs = 10  # Set the number of epochs\nloss_history = []\ncriterion = nn.CrossEntropyLoss()\ntotal_batches = len(train_loader)\n\nfor epoch in range(num_epochs):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    print(f'Starting epoch {epoch + 1} / {num_epochs}')\n\n    # Training phase\n    # Adjust the loss function\n\n\n    for batch_idx, (images, masks) in enumerate(train_loader, 1):\n        start_time = time.time()\n        images = images.to(device)\n        masks = masks.to(device)\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, masks)  # CrossEntropyLoss expects integer labels\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate training loss\n        running_loss += loss.item() * images.size(0)\n\n        # Print progress every 10 batches\n        if batch_idx % 10 == 0:\n          print(f\"Batch {batch_idx}/{total_batches} - Loss: {loss.item():.4f} - Time: {time.time() - start_time:.2f}s\")\n\n    # Average loss over training data\n    avg_training_loss = running_loss / len(train_loader.dataset)\n    loss_history.append(avg_training_loss)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {avg_training_loss:.4f}\")\n\n    # Validation phase\n    model.eval()  # Set model to evaluation mode\n    val_loss = 0.0\n    with torch.no_grad():  # No need to track gradients during validation\n        for images, masks in valid_loader:\n            images = images.to(device)\n            masks = masks.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n\n            val_loss += loss.item() * images.size(0)\n\n    avg_val_loss = val_loss / len(valid_loader.dataset)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T02:36:59.910856Z","iopub.execute_input":"2024-10-31T02:36:59.911267Z","iopub.status.idle":"2024-10-31T02:48:00.442907Z","shell.execute_reply.started":"2024-10-31T02:36:59.911230Z","shell.execute_reply":"2024-10-31T02:48:00.441689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the loss over training epochs","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(range(1, num_epochs + 1), loss_history, marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss Over Epochs')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T02:48:00.445281Z","iopub.execute_input":"2024-10-31T02:48:00.446193Z","iopub.status.idle":"2024-10-31T02:48:00.743297Z","shell.execute_reply.started":"2024-10-31T02:48:00.446154Z","shell.execute_reply":"2024-10-31T02:48:00.742455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate model on test set and plot a predicted mask.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import jaccard_score  # IoU for each class if desired\n\n# Assuming you have defined the file paths and annotations for the test set\ntest_dataset = TurtleSegmentationDataset(test_image_paths, test_annotations, image_size=(256, 256))\n\n# Set a higher batch size for faster evaluation (4 or 8 can be a good balance)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n\n# Evaluation Function\ndef evaluate_model(model, test_loader, criterion, device):\n    model.eval()  # Set model to evaluation mode\n    total_loss = 0.0\n\n    with torch.no_grad():\n        for images, masks in test_loader:\n            images = images.to(device)\n            masks = masks.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            total_loss += loss.item()\n\n    avg_loss = total_loss / len(test_loader)\n    print(f\"Average Test Loss: {avg_loss:.4f}\")\n\n# Visualization Function\ndef visualize_predictions(model, dataset, index, device):\n    model.eval()\n    image, ground_truth = dataset[index]\n    image = image.to(device).unsqueeze(0)  # Add batch dimension\n\n    with torch.no_grad():\n        output = model(image)\n        predicted_mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n\n    # Move ground truth to CPU for visualization\n    ground_truth = ground_truth.cpu().numpy()\n\n    # Plotting the results\n    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n    axs[0].imshow(image.squeeze().permute(1, 2, 0).cpu())  # Display the image\n    axs[0].set_title(\"Original Image\")\n    axs[0].axis(\"off\")\n\n    axs[1].imshow(ground_truth, cmap=\"viridis\", interpolation=\"nearest\")\n    axs[1].set_title(\"Ground Truth Mask\")\n    axs[1].axis(\"off\")\n\n    axs[2].imshow(predicted_mask, cmap=\"viridis\", interpolation=\"nearest\")\n    axs[2].set_title(\"Predicted Mask\")\n    axs[2].axis(\"off\")\n\n    plt.show()\n\n# Run evaluation\nevaluate_model(model, test_loader, criterion, device)\n\n# Visualize predictions\nsample_index = 0  # Change this index to view different test samples\nvisualize_predictions(model, test_dataset, sample_index, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T02:48:10.439998Z","iopub.execute_input":"2024-10-31T02:48:10.440714Z","iopub.status.idle":"2024-10-31T02:50:08.325290Z","shell.execute_reply.started":"2024-10-31T02:48:10.440673Z","shell.execute_reply":"2024-10-31T02:50:08.324104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Print mIoU results","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\n\n# Assuming your test dataset and model are already defined\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n\n# Define class IDs\nclass_ids = [1, 2, 3]  # head=1, turtle=2, flippers=3\n\n# Helper function to compute IoU for a specific class\ndef compute_iou(pred, target, class_id):\n    pred_binary = (pred == class_id).astype(np.uint8)\n    target_binary = (target == class_id).astype(np.uint8)\n    intersection = np.logical_and(pred_binary, target_binary).sum()\n    union = np.logical_or(pred_binary, target_binary).sum()\n    return intersection / union if union != 0 else 0\n\n# Evaluation function to calculate mIoU\ndef evaluate_miou(model, test_loader, device):\n    model.eval()\n    # Dictionaries to accumulate IoU sums and counts for each class\n    iou_sums = {class_id: 0 for class_id in class_ids}\n    iou_counts = {class_id: 0 for class_id in class_ids}\n    \n    with torch.no_grad():\n        for images, masks in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            \n            # Iterate over each sample in the batch\n            for i in range(len(outputs)):\n                pred_mask = torch.argmax(outputs[i], dim=0).cpu().numpy()  # Get predicted mask for this sample\n                target_mask = masks[i].cpu().numpy()  # Get target mask for this sample\n                \n                # Calculate IoU for each class\n                for class_id in class_ids:\n                    iou = compute_iou(pred_mask, target_mask, class_id)\n                    if iou > 0:  # Avoid counting classes not present in this image\n                        iou_sums[class_id] += iou\n                        iou_counts[class_id] += 1\n\n    # Calculate mean IoU for each class\n    iou_results = {class_id: (iou_sums[class_id] / iou_counts[class_id]) if iou_counts[class_id] > 0 else 0 for class_id in class_ids}\n    \n    # Print IoU for each class and overall mean IoU\n    print(\"\\nIntersection over Union (IoU):\")\n    class_names = [\"turtle\", \"flippers\", \"head\"]\n    for class_id, class_name in zip(class_ids, class_names):\n        print(f\"IoU for {class_name}: {iou_results[class_id]:.3f}\")\n    \n    # Calculate overall mIoU (mean of class-wise IoUs)\n    overall_miou = np.mean(list(iou_results.values()))\n    print(f\"Overall mean IoU (mIoU): {overall_miou:.3f}\")\n\n# Call the evaluation function\nevaluate_miou(model, test_loader, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T02:52:19.440986Z","iopub.execute_input":"2024-10-31T02:52:19.442015Z","iopub.status.idle":"2024-10-31T02:54:14.505608Z","shell.execute_reply.started":"2024-10-31T02:52:19.441973Z","shell.execute_reply":"2024-10-31T02:54:14.504338Z"},"trusted":true},"execution_count":null,"outputs":[]}]}